import numpy as np
import pandas as pd
import nltk
data=pd.read_csv(r'C:\Users\admincse\Documents\model\iris.csv',encoding='latin-1')
print(data)
print(data.describe())
# check for null values
print(data.isna().sum())
print(data.describe)
data.head()
data.tail(100)
# to check no of species
num=len(data[data['Species']=='Iris-versicolor'])
print('number of versicolor',num)
# to check no of species
num=len(data[data['Species']=='Iris-virginica'])
print('number of virginica',num)
# to check no of species
num=len(data[data['Species']=='Iris-setosa'])
print('number of setosa',num)
import matplotlib.pyplot as plt
fig=plt.figure()
ax=fig.add_axes([0,0,1,1])
ax.axis('equal')
l=['Iris-versicolor','Iris-virginica','Iris-setosa']
s=[50,50,50]
ax.pie(s,labels=l,autopct='%1.2f%%')
plt.show()
plt.figure(1)
plt.boxplot([data['SepalLengthCm']])
plt.figure(2)
plt.boxplot([data['SepalWidthCm']])
plt.show()
data.hist()
plt.show()
data.plot(kind='density',subplots=True , layout=(3,3),sharex=False)
import seaborn as sns
sns.pairplot(data,hue='Species');
fig=plt.gcf()
fig.set_size_inches(10,7)
fig=sns.heatmap(data.corr(),annot=True,cmap='cubehelix',linewidths=1,linecolor='k',square=True,mask=False)
x=data['SepalLengthCm'].values.reshape(-1,1)
print(x)
y=data['SepalWidthCm'].values.reshape(-1,1)
print(y)
plt.xlabel('SepalLengthCm')
plt.ylabel('SepalWidthCm')
plt.scatter(x,y,color='b')
plt.show()
corr_mat=data.corr()
print(corr_mat)
from sklearn.linear_model import LogisticRegression
from sklearn.model_selection import train_test_split
from sklearn.neighbors import KNeighborsClassifier
from sklearn import svm
from sklearn import metrics
from sklearn.tree import DecisionTreeClassifier
train,test=train_test_split(data,test_size=0.25)
print(train.shape)
print(test.shape)
train_x=train[['SepalLengthCm','SepalWidthCm','PetalLengthCm' ,'PetalWidthCm']]
train_y=train.Species

test_x=train[['SepalLengthCm','SepalWidthCm','PetalLengthCm' ,'PetalWidthCm']]
test_y=train.Species
data.head()
train_x.head()
test_y.head()
model= LogisticRegression()
model.fit(train_x,train_y)
prediction=model.predict(test_x)
print('Accuracy:',metrics.accuracy_score(prediction,test_y))
from sklearn.metrics import confusion_matrix,classification_report 
confusion_mat=confusion_matrix(test_y,prediction)
print("confusion matrix:\n",confusion_mat)
print(classification_report(test_y,prediction) )
from sklearn.svm import SVC
model1=SVC()
model1.fit(train_x,train_y)
pred_y=model1.predict(test_x)

from sklearn.metrics import accuracy_score
print("ACC=",accuracy_score(test_y,pred_y))
from sklearn.neighbors import KNeighborsClassifier
model2=KNeighborsClassifier(n_neighbors=5)
model2.fit(train_x,train_y)
pred_y2=model2.predict(test_x)
from sklearn.metrics import accuracy_score
print("ACC=",accuracy_score(test_y,pred_y2))
from sklearn.naive_bayes import GaussianNB
model3= GaussianNB()
model3.fit(train_x,train_y)
pred_y3=model2.predict(test_x)
from sklearn.metrics import accuracy_score
print("ACC=",accuracy_score(test_y,pred_y3))
from sklearn.tree import DecisionTreeClassifier
model4= DecisionTreeClassifier(criterion='entropy',random_state=7)
model4.fit(train_x,train_y)
pred_y4=model2.predict(test_x)
from sklearn.metrics import accuracy_score
print("ACC=",accuracy_score(test_y,pred_y4))
results=pd.DataFrame({
    'model1':['LogisticRegression','Support Vector Mchines','Naive Bayes','KNN','Decision Tree'],
    'Score':[0.964,0.973,0.964,0.964,0.964]
})

result_df=results.sort_values(by='Score',ascending=False)
result_df=result_df.set_index('Score')
result_df.head(9)